MODEL:
  TYPE: vssm
  NAME: vssm1_small_0229
  DROP_PATH_RATE: 0.3
  VSSM:
    EMBED_DIM: 96
    DEPTHS: [ 2, 2, 15, 2 ]
    SSM_D_STATE: 1
    SSM_DT_RANK: "auto"
    SSM_RATIO: 2.0
    SSM_CONV: 3
    SSM_CONV_BIAS: false
    SSM_FORWARDTYPE: "v05_noz" # v3_noz
    MLP_RATIO: 4.0
    DOWNSAMPLE: "v3"
    PATCHEMBED: "v2"
    NORM_LAYER: "ln2d"


def vmamba_small_s2l15(channel_first=True):
    return VSSM(
        depths=[2, 2, 15, 2], dims=96, drop_path_rate=0.3, 
        patch_size=4, in_chans=3, num_classes=1000, 
        ssm_d_state=1, ssm_ratio=2.0, ssm_dt_rank="auto", ssm_act_layer="silu",
        ssm_conv=3, ssm_conv_bias=False, ssm_drop_rate=0.0, 
        ssm_init="v0", forward_type="v05_noz", 
        mlp_ratio=4.0, mlp_act_layer="gelu", mlp_drop_rate=0.0, gmlp=False,
        patch_norm=True, norm_layer=("ln2d" if channel_first else "ln"), 
        downsample_version="v3", patchembed_version="v2", 
        use_checkpoint=False, posembed=False, imgsize=224, 
    )



def vmamba_tiny_s1l8(channel_first=True):
    return VSSM(
        depths=[2, 2, 8, 2], dims=96, drop_path_rate=0.2, 
        patch_size=4, in_chans=3, num_classes=1000, 
        ssm_d_state=1, ssm_ratio=1.0, ssm_dt_rank="auto", ssm_act_layer="silu",
        ssm_conv=3, ssm_conv_bias=False, ssm_drop_rate=0.0, 
        ssm_init="v0", forward_type="v05_noz", 
        mlp_ratio=4.0, mlp_act_layer="gelu", mlp_drop_rate=0.0, gmlp=False,
        patch_norm=True, norm_layer=("ln2d" if channel_first else "ln"), 
        downsample_version="v3", patchembed_version="v2", 
        use_checkpoint=False, posembed=False, imgsize=224, 
    )

MambaCD MODEL:
  TYPE: vssm
  NAME: vssm1_base_0229
  DROP_PATH_RATE: 0.6
  VSSM:
    EMBED_DIM: 128
    DEPTHS: [ 2, 2, 15, 2 ]
    SSM_D_STATE: 1
    SSM_DT_RANK: "auto"
    SSM_RATIO: 2.0
    SSM_CONV: 3
    SSM_CONV_BIAS: false
    SSM_FORWARDTYPE: "v3noz"
    MLP_RATIO: 4.0
    DOWNSAMPLE: "v3"
    PATCHEMBED: "v2"

# 89.0 + 15.2 + 118min/e + 48G
    
    
    

